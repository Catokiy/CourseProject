{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "571e18cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he trip began with a long flight from the mainland. Despite the length of the journey, I was excited to finally arrive in Hawaii. As soon as I stepped off the plane, I was greeted with a warm and humid climate that was a welcome change from the cold winter weather back home.  One of the first things we did upon arriving was visit the famous Waikiki Beach. The sand was soft and white, and the water was crystal clear. We spent hours swimming and soaking up the sun. We also tried our hand at surfing, which was a new and thrilling experience for me.  In addition to beach activities, we also took the time to explore the island. We hiked through the lush forests, visited the stunning waterfalls, and even drove up to the top of the island to see the breathtaking views.\n",
      "Предсказанный диагноз - Stress\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('popular', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Приведение текста к нижнему регистру\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)  # Удаление markdown-ссылок\n",
    "    text = re.sub(r'@\\w+', '', text)  # Удаление упоминаний (handle)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Удаление ссылок\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Удаление HTML-тегов\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Удаление знаков препинания\n",
    "    text = re.sub(r'\\n', '', text)  # Удаление символов новой строки\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Удаление слов с цифрами\n",
    "    return text.strip()  # Удаление лишних пробелов в начале и конце текста\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    pos_tagged = nltk.pos_tag(tokens) \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else: \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return ' '.join(lemmatized_sentence)\n",
    "\n",
    "model = pickle.load(open('model.pkl', 'rb'))\n",
    "statement = input()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = lemmatize(tokens)\n",
    "vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))\n",
    "tfidf_tokens = vectorizer.transform([lemmatized_tokens])\n",
    "additional_features = np.array([len(statement), len([i for i in lemmatized_tokens if i in ['i','ive','im','i`m','i`ve']])])\n",
    "X = hstack([tfidf_tokens, additional_features])\n",
    "labels = ['Anxiety', 'Bipolar', 'Depression', 'Normal', 'Personality disorder', 'Stress', 'Suicidal']\n",
    "print('Предсказанный диагноз - ' + labels[model.predict(X)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad20a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
